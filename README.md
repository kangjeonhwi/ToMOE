# ToMoE Implementation (Learning & Research Purpose)

This repository provides a learning-and-research reimplementation of the paper **“ToMoE: Converting Dense Large Language Models to Mixture-of-Experts through Dynamic Structural Pruning”** :contentReference[oaicite:0]{index=0}

## Paper Information
- **Title**: ToMoE: Converting Dense Large Language Models to Mixture-of-Experts through Dynamic Structural Pruning :contentReference[oaicite:1]{index=1}  
- **Authors**: Shangqian Gao, Ting Hua, Reza Shirkavand, Chi-Heng Lin, Zhen Tang, Zhengao Li, Longge Yuan, Fangyi Li, Zeyu Zhang, Alireza Ganjdanesh, Lou Qian, Xu Jie, Yen-Chang Hsu :contentReference[oaicite:2]{index=2}  
- **arXiv**: [arXiv:2501.15316](https://arxiv.org/abs/2501.15316) (submitted on 25 Jan 2025) :contentReference[oaicite:3]{index=3}  

## Implementation Purpose
- Reproduce the differentiable dynamic pruning method that converts MLP layers into a Mixture-of-Experts architecture :contentReference[oaicite:4]{index=4}  
- Evaluate ToMoE’s performance across model families (Phi-2, LLaMA-2, LLaMA-3, Qwen-2.5) without fine-tuning :contentReference[oaicite:5]{index=5}  
